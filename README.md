# RAG Pipeline Evaluation

## Introduction
This repository contains code and a report for evaluating the quality of answers generated by an AI system using various metrics. The goal is to understand the relevance, correctness, and faithfulness of the answers with respect to the provided context.

## Data Samples
The dataset consists of questions, answers, contexts (providing additional information), and ground truth (correct answers).

## Code Explanation
The provided Python code sets up a dataset, evaluates it using multiple metrics, and saves the evaluation results to a CSV file.

### Importing Libraries
```python
import os
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_correctness, context_precision, context_recall, context_entity_recall, answer_relevancy
```
- **os**: For setting environment variables.
- **datasets**: For handling datasets.
- **ragas**: For evaluating question-answering models.
- **ragas.metrics**: Contains various evaluation metrics.

### Setting the OpenAI API Key
```python
os.environ["OPENAI_API_KEY"] = "your_openai_api_key_here"
```
- Set your OpenAI API key for authenticating API requests.



### Creating a Dataset
```python
dataset = Dataset.from_dict(data_samples)
```

### Defining Metrics
```python
metrics = [
    faithfulness,
    answer_correctness,
    context_precision,
    context_recall,
    context_entity_recall,
    answer_relevancy,
]
```

### Evaluating the Dataset
```python
score = evaluate(dataset, metrics=metrics)
```

### Saving the Results
```python
df = score.to_pandas()
df.to_csv('path/to/save/score.csv', index=False)
```


